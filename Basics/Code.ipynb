{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma(z):\n",
    "    return(1 / (1 + np.exp(-z)))\n",
    "def tanh(z):\n",
    "    return((np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z)))\n",
    "def relu(z):\n",
    "    return(max(0,z))\n",
    "def leaky_relu(z):\n",
    "    return(max(0.01*z,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For one sample tuple**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogRegCompute(x_1, x_2, w_1, w_2, b, alpha,y):\n",
    "    def compute_da(y,a):\n",
    "        da = -(y/a)+(1-y)/(1-a)\n",
    "        return da\n",
    "    def compute_dz(da,a):\n",
    "        dz = da*a*(1-a)\n",
    "        return dz\n",
    "    def compute_d(dz, x=1):\n",
    "        d = dz * x\n",
    "        return d\n",
    "    \n",
    "    z = w_1*x_1 + w_2*x_2 + b\n",
    "    a = sigma(z)\n",
    "    \n",
    "    da = compute_dz(y,a)\n",
    "    dz = compute_dz(da,a)\n",
    "    \n",
    "    dw1 = compute_d(dz, x_1)\n",
    "    dw2 = compute_d(dz, x_2)\n",
    "    db = compute_d(dz)\n",
    "    \n",
    "    w_1 = w_1 + alpha*dw1\n",
    "    w_2 = w_2 + alpha*dw2\n",
    "    b = b + alpha*db\n",
    "    return(w_1,w_2,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0007731250458590582, 0.0015462500917181165, 1.0007731250458591)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogRegCompute(1,2,0,0,1,0.01,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For m samples, single step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = [83.86859737]\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "J_array, b = np.zeros((m,1)), 0\n",
    "alpha = 0.01\n",
    "np.random.seed(197)\n",
    "\n",
    "\n",
    "w = np.zeros((1,2))\n",
    "x_1 = np.random.randint(10, size = m).reshape(-1,m)\n",
    "x_2 = np.random.randint(low = 25, high = 50, size = m).reshape(-1,m)\n",
    "x = np.array([x_1,x_2]).reshape(2,m)\n",
    "\n",
    "y = np.where(((x[1]<37.5) & (x[0]>5)), 1, 0)\n",
    "\n",
    "for i in range(1000):\n",
    "    z = np.zeros(m)\n",
    "    a = np.zeros(m)\n",
    "\n",
    "    z = np.dot(w,x) + b\n",
    "    a = sigma(z)\n",
    "    J = (-(y * np.log(a) + (1-y)* np.log(1-a))).mean()\n",
    "    dz = a - y\n",
    "    dw = (np.dot(x,dz.T).reshape(-1,2))/m\n",
    "    db = dz.mean()\n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    J_array[i] = 1-J \n",
    "\n",
    "print(f'Accuracy = {J_array[m-1]*100}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy increase over 1000 iterations')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(J_array)\n",
    "plt.title(\"Accuracy increase over 1000 iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load default input/output layer config: (y/n): y\n",
      "\n",
      "Status: Output\n",
      "\n",
      "Layer 0:\n",
      "a: \n",
      "[[ 1  3  4 ...  0  9  2]\n",
      " [37 36 30 ... 28 26 35]]\n",
      "Layer 1:\n",
      "a: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "w: \n",
      "[[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "b: \n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Layer 2:\n",
      "a: \n",
      "[[0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766\n",
      "  0.2361766 0.2361766 0.2361766 0.2361766 0.2361766 0.2361766]]\n",
      "w: \n",
      "[[0. 0. 0. 0.]]\n",
      "b: \n",
      "[[-1.17411853]]\n",
      "Accuracy = [50.4170025]\n"
     ]
    }
   ],
   "source": [
    "m = 1000\n",
    "J_array, b = np.zeros((m,1)), 0\n",
    "alpha = 0.01\n",
    "np.random.seed(197)\n",
    "\n",
    "\n",
    "w = np.zeros((1,2))\n",
    "x_1 = np.random.randint(10, size = m).reshape(-1,m)\n",
    "x_2 = np.random.randint(low = 25, high = 50, size = m).reshape(-1,m)\n",
    "x = np.array([x_1,x_2]).reshape(2,m)\n",
    "\n",
    "y = np.where(((x[1]<37.5) & (x[0]>5)), 1, 0).reshape(1,1000)\n",
    "\n",
    "def init_network():\n",
    "    hidden_layer_nodes = []\n",
    "    if(input(\"Load default input/output layer config: (y/n): \").lower()=='n'):\n",
    "        input_layer_node = int(input(\"Enter number of nodes in input layer 0: \"))\n",
    "        n_layer = int(input(\"Enter number of layers in network: \"))\n",
    "        for i in range(1,n_layer):\n",
    "            hidden_layer_nodes.append(input(f\"Enter no. of nodes in layer {i}\"))\n",
    "        output_layer_node = int(input(f\"Enter number of nodes in output layer {n_layer}: \"))\n",
    "    else:\n",
    "        n_layer = 2\n",
    "        input_layer_node = 2\n",
    "        output_layer_node = 1\n",
    "        hidden_layer_nodes = [4]  \n",
    "    n__per_layer = [input_layer_node] + hidden_layer_nodes + [output_layer_node] \n",
    "    hidden_layers = {}\n",
    "    for i in range(n_layer+1):\n",
    "        hidden_layers[f'Layer {i}'] = {'a':np.zeros(shape=(n__per_layer[i],1))}\n",
    "        if(i != 0):\n",
    "            hidden_layers[f'Layer {i}']['w'] = np.zeros(shape=(n__per_layer[i],n__per_layer[i-1]))\n",
    "            hidden_layers[f'Layer {i}']['b'] = np.zeros(shape=(n__per_layer[i],1))\n",
    "    return(hidden_layers)\n",
    "\n",
    "def display_net(net, status = \"original\"):\n",
    "    print(f'\\nStatus: {status}\\n')\n",
    "    for key,value in net.items():\n",
    "        print(f'{key}:')\n",
    "        for key,value in value.items():\n",
    "            print(f'{key}: \\n{value}')\n",
    "def forward_prop(net, x):\n",
    "    n_layer = f'Layer {len(net)-1}'\n",
    "    for key,value in net.items():\n",
    "        if(key!='Layer 0'):\n",
    "            w = value['w']\n",
    "            a = value['a']\n",
    "            b = value['b']\n",
    "            if(key == n_layer):\n",
    "                a = sigma(np.dot(w,a_prev) + b)\n",
    "            else:\n",
    "                a = np.vectorize(relu)(np.dot(w,a_prev) + b)\n",
    "            value['a'] = a \n",
    "        a_prev = value['a']\n",
    "    return(net)\n",
    "def back_prop(net, y, i):\n",
    "    net = dict(reversed(list(net.items())))\n",
    "    n_layer = f'Layer {len(net)-1}'\n",
    "    J_array[i] = (-(y * np.log(net[n_layer]['a']) + (1-y)* np.log(1-net[n_layer]['a']))).mean()\n",
    "    _net = list(net.items()) \n",
    "    dz = [[] for x in range(int(n_layer[-1]))]\n",
    "    for j in range(len(_net)-1):\n",
    "        a_curr = _net[j][1]['a']\n",
    "        \n",
    "        a_next = _net[j+1][1]['a']\n",
    "        if(j == 0):\n",
    "            _dz = a_curr - y\n",
    "        else:\n",
    "            w_prev = _net[j-1][1]['w']\n",
    "            dz_prev = dz[j-1]\n",
    "            _dz = np.dot(w_prev.T,dz_prev) * a_curr*(1-a_curr)\n",
    "\n",
    "        dw = np.dot(_dz,a_next.T)/m\n",
    "        db = _dz.mean(axis = 1, keepdims = True) \n",
    "        _net[j][1]['w'] = _net[j][1]['w'] - alpha * dw \n",
    "        _net[j][1]['b'] = _net[j][1]['b'] - alpha * db\n",
    "        dz[j] = _dz\n",
    "    net = dict(reversed(_net))\n",
    "    return(net)\n",
    "\n",
    "net = init_network()\n",
    "net['Layer 0']['a'] = x\n",
    "\n",
    "for i in range(1000):\n",
    "    net = forward_prop(net, x)\n",
    "    net = back_prop(net,y, i)\n",
    "\n",
    "display_net(net, \"Output\")\n",
    "\n",
    "print(f'Accuracy = {J_array[m-1]*100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
